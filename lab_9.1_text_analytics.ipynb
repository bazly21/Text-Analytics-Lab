{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9.1\n",
    "\n",
    "Name: Muhammad Bazly Bin Burhan\n",
    "Student ID: SW01081224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kucingbunting/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kucingbunting/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kucingbunting/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For text preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# For topic modeling\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Download the NLTK resources\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "documents = [\n",
    " \"Rafael Nadal Joins Roger Federer in Missing U.S. Open\",\n",
    " \"Rafael Nadal Is Out of the Australian Open\",\n",
    " \"Biden Announces Virus Measures\",\n",
    " \"Biden's Virus Plans Meet Reality\",\n",
    " \"Where Biden's Virus Plan Stands\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Do Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['rafael', 'nadal', 'join', 'roger', 'federer', 'missing', 'open'],\n",
       " ['rafael', 'nadal', 'australian', 'open'],\n",
       " ['biden', 'announces', 'virus', 'measure'],\n",
       " ['biden', 'virus', 'plan', 'meet', 'reality'],\n",
       " ['biden', 'virus', 'plan', 'stand']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do data preprocessing on the documents\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower()) # Tokenize the text into words and convert to lowercase\n",
    "    tokens = [token for token in tokens if token.isalnum()] # Filter out non-alphanumeric tokens\n",
    "    tokens = [token for token in tokens if token not in stop_words] # Filter out stopwords\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens] # Lemmatize the tokens\n",
    "    return tokens\n",
    "\n",
    "preprocessed_documents = [preprocess_text(doc) for doc in documents]\n",
    "preprocessed_documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Crate a Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gensim Dictionary object from the preprocessed documents\n",
    "dictionary = corpora.Dictionary(preprocessed_documents)\n",
    "\n",
    "# Convert each preprocessed document into a bag-of-words representation using the dictionary\n",
    "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus: bag of words representation of the documents\n",
    "# num_topics: number of topics to be extracted by the model\n",
    "# id2word=dictionary: dictionary mapping form word IDs to words\n",
    "# passes: number of passes through the corpus during training\n",
    "\n",
    "# Train the LDA model on the corpus with 2 topics using Gensim's LdaModel class\n",
    "lda_model = LdaModel(corpus, num_topics=2, id2word=dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Interpret the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list to store dominant topic labels for each document\n",
    "article_labels = []\n",
    "\n",
    "#  Iterate over the preprocessed documents\n",
    "for i, doc in enumerate(preprocessed_documents):\n",
    "    # Fore each document, convert to box representation\n",
    "    bow = dictionary.doc2bow(doc)\n",
    "    # Get list of topic probabilities\n",
    "    topics = lda_model.get_document_topics(bow)\n",
    "    # Determine topic with highest probability\n",
    "    dominant_topic = max(topics, key=lambda x: x[1])[0]\n",
    "    # Append to the list\n",
    "    article_labels.append(dominant_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table with Articles and Topic:\n",
      "                                            Document  Topic\n",
      "0  Rafael Nadal Joins Roger Federer in Missing U....      1\n",
      "1         Rafael Nadal Is Out of the Australian Open      1\n",
      "2                     Biden Announces Virus Measures      0\n",
      "3                   Biden's Virus Plans Meet Reality      0\n",
      "4                    Where Biden's Virus Plan Stands      0\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame({'Document': documents, 'Topic': article_labels})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Table with Articles and Topic:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms for each topic:\n",
      "Topic 0:\n",
      "- \"biden\" (weight: 0.166)\n",
      "- \"virus\" (weight: 0.166)\n",
      "- \"plan\" (weight: 0.119)\n",
      "- \"reality\" (weight: 0.071)\n",
      "- \"stand\" (weight: 0.071)\n",
      "- \"measure\" (weight: 0.071)\n",
      "- \"announces\" (weight: 0.071)\n",
      "- \"meet\" (weight: 0.071)\n",
      "- \"australian\" (weight: 0.024)\n",
      "- \"open\" (weight: 0.024)\n",
      "\n",
      "Topic 1:\n",
      "- \"nadal\" (weight: 0.131)\n",
      "- \"rafael\" (weight: 0.131)\n",
      "- \"open\" (weight: 0.131)\n",
      "- \"federer\" (weight: 0.079)\n",
      "- \"join\" (weight: 0.079)\n",
      "- \"missing\" (weight: 0.079)\n",
      "- \"roger\" (weight: 0.079)\n",
      "- \"australian\" (weight: 0.079)\n",
      "- \"virus\" (weight: 0.027)\n",
      "- \"biden\" (weight: 0.027)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the top terms for each topic\n",
    "print(\"Top terms for each topic:\")\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    print(f\"Topic {idx}:\")\n",
    "    terms = [term.strip() for term in topic.split(\"+\")]\n",
    "    for term in terms:\n",
    "        weight, word = term.split(\"*\")\n",
    "        print(f\"- {word.strip()} (weight: {weight.strip()})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Calculate Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Coherence Score (C_V): 0.3801\n"
     ]
    }
   ],
   "source": [
    "# import library for Coherence Score\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Calculate the coherence score for the LDA model\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=preprocessed_documents, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "# Display the score\n",
    "print(f'Topic Coherence Score (C_V): {coherence_lda:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
